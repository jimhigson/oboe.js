<h1 id="abstract"><a href="#abstract"><span class="header-section-number">1</span> Abstract</a></h1>
<p>A Javascript REST client library targeting both Node.js and web browsers that incorporates http streaming, pattern matching, and progressive JSON parsing, with the aim of improving performance, fault tolerance, and encouraging a greater degree of loose coupling between programs. Loose coupling is particularly considered in light of the application of Agile methodologies to SOA, providing a framework in which it is acceptable to partially restructure the JSON format in which a resource is expressed whilst maintaining compatibility with dependent systems.</p>
<p>A critique is made of current practice under which resources are entirely retrieved before items of interest are extracted programmatically. An alternative model is presented allowing the specification of items of interest using a declarative syntax similar to JSONPath. The identified items are then provided incrementally while the resource is still downloading.</p>
<p>In addition to a consideration of performance in absolute terms, the usability implications of an incremental model are also evaluated with regards to differences in user perception of performance.</p>
<h1 id="introduction"><a href="#introduction"><span class="header-section-number">2</span> Introduction</a></h1>
<p>This dissertation does not focus on implementing software for any particular problem domain. Rather, its purpose is to encourage the REST paradigm to be viewed through a novel lens. In application this may be used to deliver tangible benefits to many common REST use cases. Although I express my thesis through programming, the contribution I hope to deliver is felt more strongly as a shift in how we <em>think</em> about http than it is a change in the underlying technology.</p>
<p>In the interest of developer ergonomics, REST clients have tended to style the calling of remote resources similar to the call style of the host programming language. Depending on the language, one of two schemas are followed: a synchronous style in which the http call is an expression which evaluates to the resource that was fetched; or asynchronous or monadic in which some logic is specified which may be applied to the response once it is complete. This tendency to cast REST calls using terms from the language feels quite natural; we may call a remote service without having to make any adjustment for the fact that it is remote. However, we should remember that this construct is not the only possible mapping. Importing some moderate Whorfianism<sup><a href="#fn1" class="footnoteRef" id="fnref1">1</a></sup><sup><a href="#fn2" class="footnoteRef" id="fnref2">2</a></sup> from linguistics, we might venture to say that the programming languages we use encourage us to think in the terms that they easily support. For any multi-packet message sent via a network some parts will arrive before others, at least approximately in-order, but whilst coding a C-inspired language whose return statements yield single, discrete values it comfortable to conceptualise the REST response as a discrete event. Perhaps better suited to representing a progressively returned value would have been the relatively unsupported Generator routine<sup><a href="#fn3" class="footnoteRef" id="fnref3">3</a></sup>.</p>
<p>In most practical cases where software is being used to perform a task there is no reasonable distinction between being earlier and being quicker. Therefore, if our interest is to create fast software we should be using data at the first possible opportunity. Examining data <em>while</em> it streams rather than hold unexamined until the message ends.</p>
<p>The coining of the term REST represented a shift in how we think about http, away from the transfer of hypertext documents to that of arbitrary data <sup><a href="#fn4" class="footnoteRef" id="fnref4">4</a></sup>. It introduced no fundamentally new methods. Likewise, no genuinely new computer science techniques need be invented to realise my thesis. As a minimum, the implementation requires an http client which exposes the response whilst it is in progress and a parser which can start making sense of a response before it sees all of it. I also could not claim this thesis to be an entirely novel composition of such parts. Few ideas are genuinely new and it is often wiser to mine for solved problems then to solve again afresh. The intense competition of Web browsers to be as fast as possible has already found this solution. Load any graphics rich with images -- essentially an aggregation of hypertext and images -- the HTML is parsed incrementally while it is downloading and the images are requested as soon as individual &lt;img&gt; tags are encountered. The browser's implementation involves a highly optimised parser created for a single task, that of displaying web pages. The new contribution of this dissertation is to provide a generic analog applicable to any problem domain.</p>
<h2 id="rest-aggregation-could-be-faster"><a href="#rest-aggregation-could-be-faster"><span class="header-section-number">2.1</span> REST aggregation could be faster</a></h2>
<div class="figure">
<img src="images/rest_timeline_1.png" alt="Sequence diagram showing aggregation of lower-level resources exposed via REST. A client fetches a listing of an author&#39;s publications and then the first three articles. The sequence represents the most commonly used technique in which the client does not react to the response until it is complete. In this example the second wave of requests cannot be made until after the original response is complete, at which time they are issued in quick succession. " /><p class="caption"><strong>Sequence diagram showing aggregation of lower-level resources exposed via REST.</strong> A client fetches a listing of an author's publications and then the first three articles. The sequence represents the most commonly used technique in which the client does not react to the response until it is complete. In this example the second wave of requests cannot be made until after the original response is complete, at which time they are issued in quick succession. </p>
</div>
<div class="figure">
<img src="images/rest_timeline_2.png" alt="Revised sequence of aggregation performed by a client capable of progressively interpreting the fetched resource. The client considers the response to return progressively as many small parts. Because UML sequence diagrams do not provide a concept of a returned value other than as a one-off event, the notation of lighter arrows illustrating fragments an ongoing response is introduced. Each request for an individual publication is made at the earliest possible time. As soon as the required data has been read from the original resource it is aborted rather than continue with the download of unnecessary data. This results in a moderate reduction in wait time to see all 3 articles but a dramatic reduction in waiting before reading the first content. The cadence of the right sequence has better pacing of requests with 4 being made at roughly equal intervals rather than a single request and then a rapid burst of 3. " /><p class="caption"><strong>Revised sequence of aggregation performed by a client capable of progressively interpreting the fetched resource.</strong> The client considers the response to return progressively as many small parts. Because UML sequence diagrams do not provide a concept of a returned value other than as a one-off event, the notation of lighter arrows illustrating fragments an ongoing response is introduced. Each request for an individual publication is made at the earliest possible time. As soon as the required data has been read from the original resource it is aborted rather than continue with the download of unnecessary data. This results in a moderate reduction in wait time to see all 3 articles but a dramatic reduction in waiting before reading the first content. The cadence of the right sequence has better pacing of requests with 4 being made at roughly equal intervals rather than a single request and then a rapid burst of 3. </p>
</div>
<p>Figure  and Figure  illustrate how a progressive REST client, without adjustments being required to the server may be used to display some data requested by a user sooner. While the complete data should be available to the user significantly earlier, we see a much greater improvement in how early the first piece of data is able to be displayed. This is advantageous: firstly, even if the total time to show the data were not improved, progressive display improves the perception of performance [CITEME]; secondly, a user wanting to scan from top to bottom may start reading the first article while waiting for the later ones to arrive. Alternatively, seeing the first article alone may allow the user to notice earlier that they have requested the wrong author and allow them to backtrack earlier.</p>
<p>Although the label &quot;client software&quot; in the figures above hints at software running directly on a user's own device, nodes in an n-tier architecture can rarely be placed into client and server categories in a way which is appropriate from all frames of reference. Rather, it is common for nodes to be thought of as a client from the layer below and as a server from the layer above. The advantage demonstrated holds if the aggregation existing in this layer were actually running on a server to provide a higher-level REST service than the one that it aggregates. An progressive aggregator would perform the same function and see the same benefits but simply be pushed one layer back in the stack. The progressive view of http would allow progressive response to its http request, allowing the data to be viewed similarly progressively.</p>
<h2 id="stepping-outside-the-big-small-tradeoff"><a href="#stepping-outside-the-big-small-tradeoff"><span class="header-section-number">2.2</span> Stepping outside the big-small tradeoff</a></h2>
<p>Where a domain model contains a series of data, of which ranges are made available via REST, I have often seen a trade-off with regards to how much of the series each call should request. Answering this question is usually a compromise between competing concerns in which it is not simultaneously possible to addresses all concerns satisfactorily. A good example might be a Twitter's pages listing a series of tweets where the interface designers adopted a currently trending pattern<sup><a href="#fn5" class="footnoteRef" id="fnref5">5</a></sup>, Infinite Scrolling. Starting from an initial page showing some finite number of tweets, upon scrolling to the bottom the next batch is automatically requested. The new batch is fetched in a json format and, once loaded, presented as html and added to the bottom of the page. Applied repeatedly this allows the user to scroll indefinitely, albeit punctuated by slightly jolting pauses while new content is loaded. To frame the big-small tradeoff we might consider the extreme choices. Firstly, requesting just one tweet per http request. By requesting the smallest possible content individual calls would complete very quickly and the pauses would be short. Taking the extreme small end the page stutters, pausing momentarily but frequently. Taking the opposite extreme, by requesting some huge number of tweets we see long periods of smooth scrolling partitioned by long waits.</p>
<p>I propose that my thesis may be applied used to stand down from this compromise by delivering pauses which are both infrequent and short. In the Twitter example, once we have thinking about http progressively this may be achieved quite simply by issuing large requests but instead of deferring all rendering until the request completes, render individual tweets incrementally as they are progressively parsed out of the ongoing response.</p>
<h2 id="staying-fast-on-a-fallible-network"><a href="#staying-fast-on-a-fallible-network"><span class="header-section-number">2.3</span> Staying fast on a fallible network</a></h2>
<p>We have been extremely successful in building the TCP abstraction layer over many different networks with vastly different purposes, However this means that the reliability of networks that a REST client must work with varies greatly. At one extreme we have server-room sized networks delivering data over a span of few meters with a success rate for any particular http request-response that is so high as for failure to be negligible. Occupying the opposite extreme we have mobile networks in marginal signal where it is common for downloads to be abruptly terminated due to loss of connectivity.</p>
<p>Consider an everyday situation where a user is using a phone to check their email over a mobile network whilst travelling on a train. The user prefers the simplicity of webmail so the communications are sent via REST rather than a mail specific protocol such as POP3. In this scenario the signal can be expected to be lost and reestablished many times. Whilst not strictly forbidding it, none of the web developer's standard toolkit of AJAX libraries encourage a use of the partially downloaded response if the http request fails. For example, the popular AJAX library[CITE], jQuery, very helpfully parses complete JSON or XML responses before handing back to the application. But because incomplete messages are not valid markup, on connection failure jQuery does not attempt to parse the response. Because partial responses are only available to the programmer as raw text, to handle them would involve a special case and a different methodology. Because of this difficulty I can find no example other than such messages being dropped without inspection. In practice this means that for the user checking her email, even if 90% of her inbox had been downloaded she will be shown nothing. When the network is available again the application will have to download from scratch, including the 90% which it already fetched. In this regard REST falls short of the mail-specific protocols which would display messages one at a time as they are fetched. I see much potential for improvement here.</p>
<p>Whilst of course a REST client library cannot understand the semantics of specific messages fully enough to decide if a partially downloaded message is useful. I propose that it would be an improvement to provide callbacks in such a way that the calling application may make use of partially successful messages via much the same programming as for complete messages. This fits in very well with my vision of a http response as a progressive stream of many small parts. As each part arrives it should be possible to parse and pass onto the application without knowing if the whole will be delivered successfully.</p>
<p>This style of REST client encourages an attitude of optimistic locking in the application which uses it. Upon each partial delievery of the message there may be made an implicit assumption that the whole message will be successful and as such each part can be acted on straight away. On discovering a delivery failure the application should be notified in case it should wish to rollback some of those actions. The degree of rollback could vary greatly between application domains, in the example above of a webmail client it may be that no rollback at all is performed.</p>
<h2 id="agile-methodologies-fast-deployments-and-compatibility-now-with-future-versions"><a href="#agile-methodologies-fast-deployments-and-compatibility-now-with-future-versions"><span class="header-section-number">2.4</span> Agile methodologies, fast deployments, and compatibility now with future versions</a></h2>
<p>In many respects, a SOA architecture is a good fit for the fast release cycle encouraged by Agile methodologies. Because in SOA we may consider that all data is local rather than global and that the components of the system are loosely coupled, frequent releases of any particular sub-system shouldn't pose a problem to the correct operation of the whole. Under truly Agile practice, the formats of resources exposed via REST will emerge iteratively, mirroring the iterative design of the software which reifies the message.</p>
<p>Unfortunately in practice the tools used for REST fail to encourage programming in a loosely coupled way. Working in enterprise I have often seen the release of dozens of components cancelled because a single unit failed to meet acceptance criteria, even where the failing unit contained only minor changes. Because of a tight coupling which depends on exact versions, a dense dependency graph between inter-dependent units creates the perfect environment for contagion to occur whereby the impact from a single failing unit spreads until it infects all of the system.</p>
<p>As I see it, an effective way to solve this problem would be to integrate into a REST client library the ability to use a response whilst being only loosely coupled to the <em>shape</em> of the overall message. This should be without any additional effort by the programmer as compared using message but depending on a rigidly specified overall structure. Rather than having this means of interpreting a message as an optional extra, because I believe it to be beneficial that all messages are handled this way, it should be the default means of operation for this library.</p>
<h2 id="deliverables"><a href="#deliverables"><span class="header-section-number">2.5</span> Deliverables</a></h2>
<p>To avoid feature creep I am paring down the software deliverables to the smallest area which delivers the greatest benefit. Amongst commentators on start-up culture this is known as a <em>zoom-in pivot</em> <sup><a href="#fn6" class="footnoteRef" id="fnref6">6</a></sup> and the work it produces should be the <em>Minimum Viable Product</em> or MVP[CITE], the guiding principle being that it is better to deliver a little well than more badly. By focusing tightly I cannot not deliver a full stack so I am forced to implement only solutions which interoperate in with existing stacks. This is advantageous; to somebody looking to improve an existing system, evolution is much easier to action than revolution and are therefore much more attractive.</p>
<p>To reify the vision above, a streaming client is the MVP. As we have already considered, all network transmissions are viewable though a streaming lens so an explicitly streaming server is not required. Additionally, whilst http servers capable of streaming are quite common, even if they are not typically programmed as such, I have been unable to find any example of a streaming-capable rest client.</p>
<h2 id="criteria-for-success"><a href="#criteria-for-success"><span class="header-section-number">2.6</span> Criteria for success</a></h2>
<p>Whilst the primary area of concern for this dissertation is to improve the throughput and reactivity of systems created using REST, the approach chosen may also be considered against a secondary problem often experienced in REST systems: that of tight coupling between systems and the difficulty this brings in adding new semantics to existing message formats. I find that in many cases these problems exist solely as inflexible REST client software which is unprepared to accept slight or moderate variations on previously agreed formats. Whilst loose coupling isn't the primary concern of this dissertation, I have found it to be a significant problem area and any benefit my new approach can bring to this problem should be counted towards the success of the project.</p>
<p>In evaluating this project, we may say it has been a success if non-trivial improvements in speed can be made without a corresponding increase in the difficulty of programming the client. This improvement may be in terms of a measure of the absolute time required to complete a representative task or in a user's perception of the speed in completing the task. Because applications in the target domain are much more io-bound than CPU-bound, optimisation in terms of the running time of a program on the CPU will be de-emphasised unless especially egregious. Because the improvements I am seeking are due to a more efficient use of io rather than a more optimal algorithmic expression, no distinction is made between doing something earlier and doing it faster. Indeed, for the sake of this dissertation, earlier <em>is</em> faster.</p>
<p>Because REST is often communicated over unreliable connections, my client should also allow the use of partially delivered messages without requiring programming which treats this as a special case.</p>
<p>Finally, I shall be looking at common ways in which the semantics of a message may be added to as a system is developed and examining the degree to which it is easier to program in a way which handles these unanticipated changes under my client as compared to the current common practice.</p>
<h1 id="background"><a href="#background"><span class="header-section-number">3</span> Background</a></h1>
<!---
**background should be 2-10 pages**
--->



<p>In fact, this is exactly how web browsers are implemented. However, this progressive use of http is hardwired into the browser engines rather than exposing an API suitable for general use and as such is treated as something of a special case specific to web browsers and has not so far seen a more general application. I wish to argue that a general application of this technique is viable and offers a worthwhile improvement over current common methods.</p>
<p>The above problem has many analogues and because REST uses standard web semantics applies to much more than just automated web surfing. Indeed, as the machine readability of the data increases, access early can be all the more beneficial since decisions to terminate the connection may be made. Example: academic's list of publications, then downloading all the new ones.</p>
<h2 id="some-high-level-stuff-about-webapps-and-where-processing-is-done"><a href="#some-high-level-stuff-about-webapps-and-where-processing-is-done"><span class="header-section-number">3.1</span> Some high-level stuff about webapps and where processing is done</a></h2>
<p>Ie, front-end client-side, front-end server-side.</p>
<p><a href="images/placeholder">!A webapp running with a front end generated partially on server and partially on client side</a></p>
<p>Separated from services by http calls regardless.</p>
<p>Contrast: mainframes, thin clients, X11, Wayland, PCs. NextCubes (CITE: get from old dis)</p>
<p>Next is closest pre-runner to current web architecture.</p>
<p>Twitter: Moving from client to server for performance. Reduce load times to 1 5th of what they were previously [https://blog.twitter.com/2012/improving-performance-twittercom]</p>
<p>Give static page (fairly basic but functional), then load js in the background.</p>
<p>However, with Node don't have to reengineer to move from client to server.</p>
<h2 id="where-the-delays-are-in-a-web-application"><a href="#where-the-delays-are-in-a-web-application"><span class="header-section-number">3.2</span> Where the delays are in a web application</a></h2>
<p>Even complex apps are not finding a bottleneck in javascript execution times.</p>
<ul>
<li>DOM manipulation</li>
<li>rendering</li>
<li>most significant: waiting for stuff. Going from js taking 10ms per frame to 1ms per frame will have zero difference because it is already meeting the frame rate. Might help slower devices if performance was marginal before</li>
</ul>
<blockquote>
<p>The user does something, then the app responds visually with immediacy at 30 frames per second or more, and completes a task in a few hundred milliseconds. As long as an app meets this user goal, it doesnâ€™t matter how big an abstraction layer it has to go through to get to silicon.</p>
</blockquote>
<p>http://www.sencha.com/blog/5-myths-about-mobile-web-performance/?utm_source=feedburner&amp;utm_medium=feed&amp;utm_campaign=Feed%3A+extblog+%28Ext+JS+Blog%29#date:16:00</p>
<h2 id="soa"><a href="#soa"><span class="header-section-number">3.3</span> SOA</a></h2>
<p>REST/WebServices (WSDL etc)</p>
<p>What is a rest client in this context (a client library)</p>
<p>Marshalling/ de-marshalling. Benefits and the problems that it causes. Allows one model to be written out to XML or JSON</p>
<p>Big/small message problem and granularity. With small: http overhead. With big: not all may be needed.</p>
<p>Javascript as mis-understood language (CITE: Crockford) - list features available.</p>
<p>(correctly, ECMAScript) Misleadingly named after Java as a marketing ploy when Java was a new technology (CITE) - in true more similar to Scheme or Lisp but with Java or C inspired syntax.</p>
<h2 id="anatomy-of-a-soa-client"><a href="#anatomy-of-a-soa-client"><span class="header-section-number">3.4</span> Anatomy of a SOA client</a></h2>
<p>First stage after getting a resource is usually to programmatically extract the interesting part from it. This is usually done via calls in the programming language itself, for example by de-marshaling the stream to domain objects and then calling a series of getters to narrow down to the interesting parts.</p>
<p>This part has become such a natural component of a workflow that it is barely noticed that it is happening. In an OO language, the extraction of small parts of a model which, in the scope of the current concern are of interest is so universal that it could be considered the sole reason that getters exist.</p>
<p>However subtly incorporated it has become in the thinking of the programmer, we should note that this is a construct and only one possible way of thinking regarding identifying the areas of current interest in a wider model.</p>
<pre class="sourceCode java"><code class="sourceCode java"><span class="co">// an example programmatic approach to a domain model interrogation under Java</span>

List&lt;Person&gt; people = myModel.<span class="fu">getPeople</span>();
String firstPersonsSurname = people.<span class="fu">get</span>(<span class="dv">0</span>).<span class="fu">getSurname</span>();</code></pre>
<p>One weakness of this imperative, programatic inspection model is that, once much code is written to interogate models in this way, the interface of the model becomes increasingly expensive to change as the code making the inspections becomes more tightly coupled with the thing that it is inspecting. Taking the above example, if the model were later refactored such that the concepts of firstName and surName were pulled from the Person class into an extracted Name class, because the inspection relies on a sequence of calls made directly into domain objects, the code making the query would also have to change.</p>
<p>I believe that this coupling defies Agile methods of programming. Many Java IDEs provide tools that would offer to automate the above extraction into a Name class, creating the new class and altering the existing calls. While reducing the pain, if we accept the concept as I stated in the <a href="#introduction">Introduction</a> that the code should not be seen as a static thing in which understanding is</p>
<p>More declarative syntaxes exist which are flexible enough that the declarative expressions may still apply as the underlying model is refactored. Whilst not applicable to use in general purpose programming, XPATH is an example of this. As an analogue of the Java situation above, Given the following XML:</p>
<pre class="sourceCode xml"><code class="sourceCode xml"><span class="kw">&lt;people&gt;</span>
   <span class="kw">&lt;person&gt;</span>
      <span class="kw">&lt;surname&gt;</span>Bond<span class="kw">&lt;/surname&gt;</span>
   <span class="kw">&lt;/person&gt;</span>
<span class="kw">&lt;/people&gt;</span></code></pre>
<p>The XPath //person[0]//surname//text() (JIM/ME - CHECK THIS!) would continue to identify the correct part of the resource without being updated after the xml analogue of the above Java Name refactor:</p>
<pre class="sourceCode xml"><code class="sourceCode xml"><span class="kw">&lt;people&gt;</span>
   <span class="kw">&lt;person&gt;</span>
      <span class="kw">&lt;name&gt;</span>
         <span class="kw">&lt;surname&gt;</span>Bond<span class="kw">&lt;/surname&gt;</span>
      <span class="kw">&lt;/name&gt;</span>
   <span class="kw">&lt;/person&gt;</span>
<span class="kw">&lt;/people&gt;</span></code></pre>
<p>A few models exist which do not follow this pattern such as XPATH. However, these are useful in only a small domain.</p>
<p>Xpath is able to express identifiers which often survive refactoring because XML represents a tree, hence we can consider relationships between entities to be that of contains/contained in (also siblings?). In application of XML, in the languages that we build on top of XML, it is very natural to consider all elements to belong to their ancestors. Examples are myriad, for example consider a word count in a book written in DOCBook format - it should be calculable without knowing if the book is split into chapters or not since this is a concept internal to the oranisation of the book itserlf nd not soemthing that a querier is likely to find interesting - if this must be considered the structure acts as barrier to information rather than enabling the information's delivery. Therefore, in many cases the exact location of a piece of information is not as important as a more general location of x being in some way under y.</p>
<p>This may not always hold. A slightly contrived example might be if we were representing a model of partial knowledge:</p>
<pre class="sourceCode xml"><code class="sourceCode xml"><span class="kw">&lt;people&gt;</span>
   <span class="kw">&lt;person&gt;</span>
      <span class="kw">&lt;name&gt;</span>
         <span class="kw">&lt;isNot&gt;&lt;surname&gt;</span>Bond<span class="kw">&lt;/surname&gt;&lt;/isNot&gt;</span>
      <span class="kw">&lt;/name&gt;</span>
   <span class="kw">&lt;/person&gt;</span>
<span class="kw">&lt;/people&gt;</span></code></pre>
<p>CSS. Meant for presentation of HTML, but where HTML markup is semantic it is a selector of the <em>meaning of elements</em> for the sake of applying a meaningful presentation more so than a selector of arbitrary colours and positions on a screen.</p>
<p>Unlike XML, in the model created by most general programming languages, there is no requirement for the data to be tree shaped. Graph is ok. This make this slighlty harder but nontheless attempts have been made.</p>
<p>Linq. (CITEME)</p>
<h2 id="parsing-sax-and-dom"><a href="#parsing-sax-and-dom"><span class="header-section-number">3.5</span> Parsing: SAX and Dom</a></h2>
<p>Why sax is difficult</p>
<p>DOM parser can be built on a SAX parser</p>
<h2 id="state-of-http-as-a-streaming-technology"><a href="#state-of-http-as-a-streaming-technology"><span class="header-section-number">3.6</span> State of http as a streaming technology</a></h2>
<p>Http libraries feeding into the parser. In browser, generally single callback when whole message received.</p>
<p>Client-side web scripting via Javascript is a field which at inception contributed no more than small, frequently gimmicky, dynamic features added to otherwise static webpages. Today the scope and power of client side scripting has increased to the extent that the entire interface for large, complex applications is often programmed in this way. These applications are not limited to running under traditional web browsers but also include mobile apps and desktop software.</p>
<p>Dichotamy between streaming and downloading in the browser for downloading data. But not for html (progressive rendering) or images (progressive PNGs and progressive JPEGs).</p>
<p>Also progressive SVGs. IE, load this in a recent version of Google Chrome: [https://upload.wikimedia.org/wikipedia/commons/0/04/Marriage_%28Same-Sex_Couples%29_Bill%2C_Second_Reading.svg]</p>
<p>Lack of support in browser Long poll - for infrequent push messages. Must be read Writing script tags</p>
<p>All require server to have a special mode. Encoding is specific to get arround restrictions.</p>
<p>JsonPath in general tries to resemble the javascript use of the json language nodes it is detecting.</p>
<pre class="sourceCode javascript"><code class="sourceCode javascript">
<span class="co">// an in-memory person with a multi-line address:</span>
<span class="kw">let</span> person = {
   <span class="dt">name</span>: <span class="st">&quot;...&quot;</span>,
   <span class="dt">address</span>: [
      <span class="st">&quot;line1&quot;</span>,
      <span class="st">&quot;line2&quot;</span>,
      <span class="st">&quot;line3&quot;</span>
   ]
};


<span class="co">// in javascript we can get line two of the address as such:</span>
<span class="kw">let</span> addresss = <span class="ot">person</span>.<span class="fu">address</span>[<span class="dv">2</span>]

<span class="co">// the equivalent jsonpath expression is identical:</span>
<span class="kw">let</span> jsonPath = <span class="st">&quot;person.address[2]&quot;</span></code></pre>
<p>What 'this' (context) is in javascript. Why not calling it scope.</p>
<h2 id="the-web-browser-as-rest-client"><a href="#the-web-browser-as-rest-client"><span class="header-section-number">3.7</span> The web browser as REST client</a></h2>
<p>Browser incompatability mostly in presentation layer rather than in scripting languages.</p>
<p>Language grammars rarely disagree, incompatability due to scripting is almost always due to the APIs presented to the scripting language rather than the language itself.</p>
<h2 id="progressive-ui"><a href="#progressive-ui"><span class="header-section-number">3.8</span> Progressive UI</a></h2>
<p>Infinitely scrolling webpages. Need a way to 'pull' information, not just push if reacting to scrolling. Use oboe with websockets? Eg, ebay home page, Facebook. Adv of infinate scroll is page loads quickly and most people won't scroll very far so most of the time have everything needed right away.</p>
<h2 id="state-of-rest-json-and-xml"><a href="#state-of-rest-json-and-xml"><span class="header-section-number">3.9</span> State of rest: Json and XML</a></h2>
<p>Json is very simple, only a few CFGs required to describe the language (json.org) - this project is listed there!</p>
<h2 id="javascript"><a href="#javascript"><span class="header-section-number">3.10</span> Javascript</a></h2>
<p>Javascript: not the greatest for 'final' elegant presentation of programming. Does allow 'messy' first drafts which can be refactored into beautiful code. Ie, can write stateful and refactor in small steps towards being stateless. An awareness of beautiful languages lets us know the right direction to go in. An ugly language lets us find something easy to write that works to get us started. Allows a very sketchy program to be written, little more than a programming scratchpad.</p>
<p>Without strict typing, hard to know if program is correct without running it. In theory (decidability) and in practice (often find errors through running and finding errors thrown). Echo FPR: once compiling, good typing tends to give a reasonable sureness that the code is correct.</p>
<p>Explain var/function difference, ie construct pluck and explain why var keyOf = partial(pluck) is declared with a var and not a function, why some prefer to do always via . operator can't be made into a function with (.) or similar and so has to be wrapped in a function is a less direct manner. Unfortunately, can make it difficult for a reader to know the types involved. For example, on seeing: <code>var matchesJsonPath = jsonPathCompiler( pattern )</code> there is no way (other than examining the source or doucmentation of the function being called) to know that this is a higher order function and will return another function to be assigned as matchesJsonPath.</p>
<p>C-style brackets around all function arguments hampers a natural expression of functional style code. For example, this requires a lot of arguments and without checking of function airity, it is easy to misplace a comma or closing bracket.</p>
<pre><code>function map(fn, list){
   if( !list ) {
      return emptyList;
   } else {
      return cons(fn(head(list)), map(fn,tail(list)));
   }
}</code></pre>
<h2 id="node"><a href="#node"><span class="header-section-number">3.11</span> Node</a></h2>
<blockquote>
<p>Streams in node are one of the rare occasions when doing something the fast way is actually easier. SO USE THEM. not since bash has streaming been introduced into a high level language as nicely as it is in node.&quot; <a href="https://gist.github.com/2401787">high level node style guide</a></p>
</blockquote>
<blockquote>
<p>node Stream API, which is the core I/O abstraction in Node.js (which is a tool for I/O) is essentially an abstract in/out interface that can handle any protocol/stream that also happens to be written in JavaScript. [http://maxogden.com/a-proposal-for-streaming-xhr.html]</p>
</blockquote>
<p>Bash streams a powerful abstraction easily programmed for linear streaming. Node more powerful, allows a powerful streaming abstraction which is no more complex to program than a javascript webapp front end. Essentially a low-level interface to streaming such as unix sockets or tcp connections.</p>
<p>Streams in node are the observer pattern. Readable streams emit 'readable' events when they have some data to be read and 'end' events when they are finished. Apart from error handling, so far as reading is concerned, that is the extent of the API.</p>
<p>Although the streams themselves are stateful, because they are based on callbacks it is entirely possible to use them from a component of a javascript program which is wholly stateless.</p>
<p>Using Node's http module provides a stream but handles setting headers, putting the method otu etc.</p>
<p>What Node is V8. Fast. Near-native. JIT.</p>
<p>V8 is often said to be 'near-native' speed, meaning it runs at close to the speed of a similarly coded C program. However, this relies on the programmer also coding in the style of a C programmer, for example with only mono-morphic callsites and without a functional style. Once either of those programming techniques is taken up performance drops rapidly [http://rfrn.org/~shu/2013/03/20/two-reasons-functional-style-is-slow-in-spidermonkey.html]. When used in a functional style, not 'near-native' in the sense that not close to the performance gained by compiling a well designed functional language to natively executable code. Depends on style coded in, comparison to native somewhat takes C as the description of the operation of an idealised CPU rather than an abstract machine capable of executing on an actual CPU.</p>
<p><em>Anecdote: SVG engine: one function for xy vs x and xy. Very large speed increase. Add figures etc.</em></p>
<p>Why Node perhaps is mis-placed in its current usage as a purely web platform &quot;the aim is absolutely fast io&quot;. This happened because web specialist programmers took it up first</p>
<p>Why Node is significant * Recognises that most tasks are io-bound rather than CPU bound. Threaded models good for CPU-bound in the main.</p>
<p>How Node is different</p>
<p>Criticisms of Node. Esp from Erlang etc devs.</p>
<p>Node's standard stream mechanisms</p>
<h2 id="browser"><a href="#browser"><span class="header-section-number">3.12</span> Browser</a></h2>
<p><em>XmlHttpRequest</em> (XHR)</p>
<p>Xhr2 and the .onprogress callback. polling responseText while in progress * why doesn't work in IE (built on an activeX object that provides buffering)</p>
<p>Older style of javascript callback. Assign a listener to onprogress, not call an add listener method means can only have one listener.</p>
<blockquote>
<p>While the request entity body is being transmitted and the upload complete flag is unset, queue a task to fire a progress event named progress on the XMLHttpRequestUpload object about every 50ms or for every byte transmitted, whichever is least frequent. <a href="http://www.w3.org/TR/XMLHttpRequest/">w3c, XHR Working Draft</a></p>
</blockquote>
<p>Websockets More like node Can connect to any protocol (as always, easier to program if text based but can do binary) Can use to do http but not sufficient advantage over using</p>
<h2 id="http-resume"><a href="#http-resume"><span class="header-section-number">3.13</span> Http resume</a></h2>
<p>Http 1.1 provides a mechanism for Byte Serving via the Accepts-Ranges header [http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.5] which can be used to request any contiguous part of a response rather than the whole. Common in download managers but not REST clients. This ability can be used to</p>
<p>Why not this one. Resume on a higher semantic level.</p>
<h2 id="updating-versioning"><a href="#updating-versioning"><span class="header-section-number">3.14</span> Updating versioning</a></h2>
<p>Because of the contagion problem, need to be able to create loosely-coupled systems.</p>
<p>Inside systems also, even with automatic refactoring tools, only automate and therefoer lessen but do not remove the problem that coupling causes changes in one place of a codebase to cause knock-on changes in remote other parts of the code. A method of programming which was truly compatible with extreme programming would involve designing for constant change without disparate parts having to be modified as structural refactoring occurs.</p>
<p>I propose that in a changing system, readability of code's changelog is as important as readability of the code itself. Extraneous changes dilute the changelog, making it less easily defined by code changes which are intrinsically linked to the actual change in the logic being expressed by the program.</p>
<p>It is often stated that understandability is the number once most important concern in a codebase (CITE) - if the code is suitably dynamic it is important that changes are axiomic and clarity of the changelog is equally important.</p>
<h2 id="micro-libraries"><a href="#micro-libraries"><span class="header-section-number">3.15</span> Micro-libraries</a></h2>
<p>What a Micro-library is. What motivates the trend? This library has a fairly small set of functionality, it isn't a general purpose do-everything library like jQuery so its size will be looked at more critically if it is too large. Micro library is the current gold standard for compactness. Still, have a lot to do in not very much code.</p>
<h2 id="methodology"><a href="#methodology"><span class="header-section-number">3.16</span> Methodology</a></h2>
<p>The program design will be initially an exercise in creating the easiest expression that can possibly work and via constant work towards the emergence of elegance.</p>
<p>Why this method? See W'yg.</p>
<h2 id="testing"><a href="#testing"><span class="header-section-number">3.17</span> Testing</a></h2>
<div class="figure">
<img src="images/placeholder.png" alt="Relationship between the main players in the JS testing landscape. JSTD, Karma, Jasmine, NodeUnit, jasmine-node, Browsers" /><p class="caption">Relationship between the main players in the JS testing landscape. JSTD, Karma, Jasmine, NodeUnit, jasmine-node, Browsers</p>
</div>
<p>By the commonjs spec, test directory should be called 'test' (http://wiki.commonjs.org/wiki/Packages/1.0#Package_Directory_Layout) doesn't matter for my project since not using commonjs, but might as well stick to the convention.</p>
<p>How TDD helps How can fit into methodology</p>
<ul>
<li>JSTD</li>
<li>NodeUnit</li>
<li>Karma</li>
<li>Jasmine</li>
</ul>
<p>Initially started with jstestdriver but found it difficult. Karma started because engineers working on the Angular project in Google were &quot;struggling a lot with jstd&quot;: http://www.youtube.com/watch?v=MVw8N3hTfCI - jstd is a google project Even Jstd's authors seems to be disowning it slightly. Describe what was once its main mode of operation as now being for stress testing of jstd itself only. Problems: browsers become unresponsive. Generally unreliable, has to be restarted frequently.</p>
<p>JSTD, as a Java program, is difficult to start via Grunt. Also an issue that Grunt post-dates Karma by enough that JSTD doesn't have the attention of the Grunt community.</p>
<h1 id="application-and-reflection"><a href="#application-and-reflection"><span class="header-section-number">4</span> Application and Reflection</a></h1>
<!---
**40 to 60 pages**
--->

<p>A feature set which is minimal but contain no obvious omissions.</p>
<p>Under the heading <a href="#anatomy-of-a-soa-client">Anatomy of a SOA client</a> I deconstructed the way in which programming logic is often used to identify the parts of a model which are currently interesting and started to look at some declarative ways in which these parts can be obtained.</p>
<p>Turn this model inside out. Instead of the programmer finding the parts they want as a part of the general logic of the program, declaratively define the interesting parts and have these parts delivered to the language logic. Once we make the shift to thinking in this way, it is no longer necessary to have the whole resource locally before the interesting sub-parts are delivered.</p>
<p>Focus on replacing ajax, rather than streaming. In older browsers, getting the whole message at once is no worse than it is now.</p>
<div class="figure">
<img src="images/timeline.png" alt="Over several hops of aggregation, the benefits of finding the interesting parts early" /><p class="caption">Over several hops of aggregation, the benefits of finding the interesting parts early</p>
</div>
<h2 id="delivery-methodology"><a href="#delivery-methodology"><span class="header-section-number">4.1</span> delivery methodology</a></h2>
<p>Because Kanban focusses on always having a potentially releasable product, it mitigates problems which could otherwise lead to non-delivery and allows the direction to be changed while the project is in progress. For each unit of work (under Kanban, a card), an entire vertical slice of planning, design, implementation and reflection must be complete before going onto the next card. Alongside each software feature, every written chapter will be expanded and refactored in much the same way as the code. Just as for well designed software, the order of implementation should not be apparent to a user, my plan is that the written work should not feel disjointed for having been written non-sequentially. I plan to manage the Kanban process using paper only, with cards on a physical board.</p>
<h2 id="overall-design-philosophy-and-breaking-out-of-bigsmall-tradeoff"><a href="#overall-design-philosophy-and-breaking-out-of-bigsmall-tradeoff"><span class="header-section-number">4.2</span> overall design philosophy and breaking out of big/small tradeoff</a></h2>
<p>In which a callback call is received not just when the whole resource is downloaded but for every interesting part which is seen while the transfer is ongoing. The definition of 'interesting' will be generic and accommodating enough so as to apply to any data domain and allow any granularity of interest, from large object to individual datums. With just a few lines of programming</p>
<p>Best of both modes</p>
<p>Granularity: only need read as far as necessary. Services could be designed to write the big picture first. Alternatively, where resources link to one another, can stop reading at the link. Eg, looking for a person's publications, start with an index of people but no need to read whole list of people.</p>
<p>Aborting http request may not stop processing on the server. Why this is perhaps desirable - transactions, leaving resources in a half-complete state.</p>
<h2 id="choice-of-technologies"><a href="#choice-of-technologies"><span class="header-section-number">4.3</span> choice of technologies</a></h2>
<p>can justify why js as:</p>
<p>Most widely deployable.</p>
<p>Node: asynchronous model built into language already, no 'concurrent' library needed. Closures convenient for picking up again where left off.</p>
<p>Node programs often so asynchronous and callback based they become unclear in structure. Promises approach to avoid pyramid-shaped code and callback spaghetti.</p>
<pre class="sourceCode javascript"><code class="sourceCode javascript"><span class="co">// example of pyramid code</span></code></pre>
<p>In comparison to typical Tomcat-style threading model. Threaded model is powerful for genuine parallel computation but Wasteful of resources where the tasks are more io-bound than cpu-bound. Resources consumed by threads while doing nothing but waiting.</p>
<p>Compare to Erlang. Waiter model. Node resturaunt much more efficient use of expensive resources.</p>
<p>funcitonal, pure functional possible [FPR] but not as nicely as in a pure functional language, ie function caches although can be implemented, not universal on all functions.</p>
<p>easy to distribute softare (npm etc)</p>
<h2 id="summary-of-json"><a href="#summary-of-json"><span class="header-section-number">4.4</span> summary of json</a></h2>
<p>Why json?</p>
<p>A bridge between languages that isn't too different from the common types in the languages themselves a common bridge between languages</p>
<p>Also very simple. Easy to parse.</p>
<h2 id="creating-a-losely-coupled-reader"><a href="#creating-a-losely-coupled-reader"><span class="header-section-number">4.5</span> creating a losely coupled reader</a></h2>
<p>Programming to identify a certain interesting part of a resource today should with a high probability still work when applied to future releases.</p>
<p>Requires a small amount of discipline on behalf of the service provider: Upgrade by adding of semantics only most of the time rather than changing existing semantics.</p>
<p>Adding of semantics should could include adding new fields to objects (which could themselves contain large sub-trees) or a &quot;push-down&quot; refactor in which what was a root node is pushed down a level by being suspended from a new parent. See </p>
<div class="figure">
<img src="images/placeholder.png" alt="extended json rest service that still works - maybe do a table instead " /><p class="caption">extended json rest service that still works - maybe do a table instead </p>
</div>
<p>(CITE: re-read citations from SOA)</p>
<h2 id="design-of-the-jsonpath-parser"><a href="#design-of-the-jsonpath-parser"><span class="header-section-number">4.6</span> Design of the jsonpath parser</a></h2>
<p>Explain why Haskel/lisp style lists are used rather than arrays</p>
<ul>
<li>In parser clauses, lots of 'do this then go to the next function with the rest'.</li>
<li>Normal arrays extremely inefficient to make a copy with one item popped off the start</li>
<li>Link to FastList on github</li>
<li>For sake of micro-library, implemented tiny list code with very bare needed</li>
<li>Alternative (first impl) was to pass an index around</li>
<li>But clause fns don't really care about indexes, they care about top of the list.</li>
<li>Slight advantage to index: allows going past the start for the root path (which doesn't have any index) instead, have to use a special value to keep node and path list of the same length</li>
<li>Special token for root, takes advantage of object identity to make certain that cannot clash with something from the json. Better than '<strong>root</strong>' or similar which could clash. String in js not considered distinct, any two strings with identical character sequences are indistinguishable.</li>
</ul>
<p>Anti-list: nothing is quite so small when making a mircro-library as using the types built into the language, coming as they are for zero bytes.</p>
<div class="figure">
<img src="images/placeholder.png" alt="Diagram showing why list is more memory efficient - multiple handles into same structure with different starts, contrast with same as an array" /><p class="caption">Diagram showing why list is more memory efficient - multiple handles into same structure with different starts, contrast with same as an array</p>
</div>
<ul>
<li>For recognisably with existing code, use lists internally but transform into array on the boundary between Oboe.js and the outside world (at same time, strip off special 'root path' token)</li>
</ul>
<p>In parser, can't use 'y' flag to the regualr expression engine which would allow much more elegant matching. Only alternative is cumersome: to slice the string and match all tokens with regexes starting with '^' in order to track the current location. [https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Regular_Expressions]</p>
<h2 id="incrementally-building-up-the-content"><a href="#incrementally-building-up-the-content"><span class="header-section-number">4.7</span> Incrementally building up the content</a></h2>
<p>Like SAX, calls from clarinet are entirely 'context free'. Ie, am told that there is a new object but without the preceding calls the root object is indistinguishable from a deeply nested object. Luckily, it should be easy to see that building up this context is a simple matter of maintaining a stack describing the descent from the root node to the current node.</p>
<p>jsonPath parser gets the output from the incrementalParsedContent, minimally routed there by the controller.</p>
<div class="figure">
<img src="images/placeholder.png" alt="Show a call into a compiled jsonPath to explain coming from incrementalParsedContent with two lists, ie the paths and the objects and how they relate to each other. Can use links to show that object list contains objects that contain others on the list. Aubergine etc example might be a good one" /><p class="caption">Show a call into a compiled jsonPath to explain coming from incrementalParsedContent with two lists, ie the paths and the objects and how they relate to each other. Can use links to show that object list contains objects that contain others on the list. Aubergine etc example might be a good one</p>
</div>
<p>Explain match starting from end of candidate path</p>
<div class="figure">
<img src="images/placeholder.png" alt="Some kind of diagram showing jsonPath expressions and functions partially completed to link back to the previous function. Include the statementExpr pointing to the last clause" /><p class="caption">Some kind of diagram showing jsonPath expressions and functions partially completed to link back to the previous function. Include the statementExpr pointing to the last clause</p>
</div>
<p>On first attempt at ICB, had two stacks, both arrays, plus reference to current node, current key and root node. After refactorings, just one list was enough. Why single-argument functions are helpful (composition etc)</p>
<p>Stateless makes using a debugger easier - can look back in stack trace and because of no reassignment, can see the whole, unchanged state of the parent call. What the params are now are what they always have been, no chance of reassignment (some code style guides recommend not to reassign parameters but imperative languages generally do not forbid it) No Side effects: can type expressions into debugger to see evaluation without risk of changing program execution.</p>
<h2 id="identifying-interesting-objects-in-the-stream"><a href="#identifying-interesting-objects-in-the-stream"><span class="header-section-number">4.8</span> identifying interesting objects in the stream</a></h2>
<p>NB: This consideration of type in json could be in the Background section.</p>
<p>Xml comes with a strong concept of the <em>type</em> of an element, the tag name is taken as a more immediate fundamental property of the thing than the attributes. For example, in automatic json-Java object demarshallers, the tag name is always mapped to the Java class. In JSON, other than the base types common to most languages (array, object, string etc) there is no further concept of type. If we wish to build a further understanding of the type of the objects then the realtionship with the parent object, expressed by the attribute name, is more likely to indicate the type. A second approach is to use duck typing in which the relationship of the object to its ancestors is not examined but the properties of the object are used instead to communicate an enhanced concept of type. For example, we might say that any object with an isbn and a title is a book.</p>
<p>Duck typing is of course a much looser concept than an XML document's tag names and collisions are possible where objects co-incidentally share property names. In practice however, I find the looseness a strength more often than a weakness. Under a tag-based marshalling from an OO language, sub-types are assigned a new tag name and as a consumer of the document, the 'isa' relationship between a 'class' tagname and it's 'sub-tabname' may be difficult to track. It is likely that if I'm unaware of this, I'm not interested in the extended capabilities of the subclass and would rather just continue to recieve the base superclass capabilities as before. Under duck typing this is easy - becase the data consumer lists the</p>
<p>A third injection of type into json comes in the form of taking the first property of an object as being the tagname. Unsatisfactory, objects have an order while serialised as json but once deserialised typically have no further order. Clarinet.js seems to follow this pattern, notifying of new objects only once the first property's key is known so that it may be used to infer type. Can't be used with a general-purpose JSON writer tool, nor any JSON writer tool that reads from common objects.</p>
<p>Design not just for now, design to be stable over future iterations of the software. Agile etc.</p>
<p>Why an existing jsonPath implmentation couldn't be used: need to add new features and need to be able to check against a path expressed as a stack of nodes.</p>
<p>More important to efficiently detect or efficiently compile the patterns?</p>
<p>The failure of sax: requires programmer to do a lot of work to identify interesting things. Eg, to find tag address inside tag person with a given name, have to recognise three things while reieving a callback for every single element and attribute in the document. As a principle, the programmer should only have to handle the cases which are interesting to them, not wade manually through a haystack in search of a needle, which means the library should provide an expressive way of associating the nodes of interest with their targetted callbacks.</p>
<p>First way to identify an interesting thing is by its location in the document. In the absense of node typing beyond the categorisation as objects, arrays and various primative types, the key immediately mapping to the object is often taken as a lose concept of the type of the object. Quite fortunately, rather than because of a well considered object design, this tends to play well with automatically marshaling of domain objects expressed in a Java-style OO language because there is a stong tendency for field names -- and by extension, 'get' methods -- to be named after the <em>type</em> of the field, the name of the type also serving as a rough summary of the relationship between two objects. See figure  below.</p>
<div class="figure">
<img src="images/marshall.png" alt="UML class diagram showing a person class in relationship with an address class. In implementation as Java the &#39;hasAddress&#39; relationship would typically be reified as a getAddress method. This co-incidence of object type and the name of the field referring to the type lends itself well to the tendency for the immediate key before an object to be taken as the type when Java models are marshaled into json " /><p class="caption">UML class diagram showing a person class in relationship with an address class. In implementation as Java the 'hasAddress' relationship would typically be reified as a getAddress method. This co-incidence of object type and the name of the field referring to the type lends itself well to the tendency for the immediate key before an object to be taken as the type when Java models are marshaled into json </p>
</div>
<p>By sensible convention, even in a serialisation format with only a loose definition of lists, lists contain only items of the same type. This gives way to a sister convention, that for lists of items, the key immediately linking to the</p>
<p>Essentially two ways to identify an interesting node - by location (covered by existing jsonpath)</p>
<p>Why duck typing is desirable in absense of genuine types in the json standard (ala tag names in XML). or by a loose concept of type which is not well supported by existing jsonpath spec.</p>
<p>Compare duck typing to the tag name in</p>
<p>To extend JsonPath to support a concise expression of duck typing, I chose a syntax which is similar to fields in jsonFormat:</p>
<pre class="sourceCode javascript"><code class="sourceCode javascript">
<span class="co">// the curly braces are my extension to jsonpath&quot;</span>
<span class="kw">let</span> jsonPath = <span class="fu">jsonPathCompiler</span>(<span class="st">&quot;{name, address, email}&quot;</span>);

<span class="co">// the above jsonPath expression would match this object in json expression and </span>
<span class="co">// like all json path expressions the pattern is quite similar to the object that</span>
<span class="co">// it matches. The object below matches because it contains all the fields listed</span>
<span class="co">// in between the curly braces in the above json path expresson.</span>

<span class="kw">let</span> matchingObject = {
   <span class="st">&quot;name&quot;</span>: <span class="st">&quot;...&quot;</span>,
   <span class="st">&quot;address&quot;</span>: <span class="st">&quot;...&quot;</span>,
   <span class="st">&quot;email&quot;</span>: <span class="st">&quot;...:</span>
}

<span class="fu">jsonPath</span>(matchingObject); <span class="co">// evaluates to true</span></code></pre>
<p>When we aer searching</p>
<h2 id="program-design"><a href="#program-design"><span class="header-section-number">4.9</span> program design</a></h2>
<div class="figure">
<img src="images/overallDesign.png" alt="Overall design of Oboe.js. Nodes in the diagram represent division of control so far that it has been split into different files." /><p class="caption">Overall design of Oboe.js. Nodes in the diagram represent division of control so far that it has been split into different files.</p>
</div>
<h2 id="incrementally-building-up-a-model"><a href="#incrementally-building-up-a-model"><span class="header-section-number">4.10</span> incrementally building up a model</a></h2>
<p>A refactoring was used to separate logic and state:</p>
<ul>
<li>Take stateful code</li>
<li>Refactor until there is just one stateful item</li>
<li>This means that that item is reassigned rather than mutated</li>
<li>Make stateless by making all functions take and return an instance of that item<br /></li>
<li>Replace all assignment of the single stateful var with a return statement</li>
<li>Create a simple, separate stateful controller that just updates the state to that returned from the calls</li>
</ul>
<p>Very testable code because stateless - once correct for params under test, will always be correct. Nowhere for bad data to hide in the program.</p>
<p>How do notifications fit into this?</p>
<p>By going to List-style, enforced that functions fail when not able to give an answer. Js default is to return the special 'undefined' value. Why this ensured more robustness but also sometimes took more code to write, ie couldn't just do if( tail(foo)) if foo could be empty but most of the time that would be correct</p>
<p>Stateful controller very easy to test - only 1 function.</p>
<h2 id="styles-of-programming"><a href="#styles-of-programming"><span class="header-section-number">4.11</span> styles of programming</a></h2>
<p>The code presented is the result of the development many prior versions, it has never been rewritten in the sense of starting again. Nontheless, every part has been complely renewed several times. I am reviewing only the final version. Git promotes regular commits, there have been more than 500.</p>
<p>some of it is pure functional (jsonPath, controller) ie, only semantically different from a Haskell programme others, syntactically functional but stateful to fit in with expected APIs etc</p>
<p>JsonPath implementation allows the compilation of complex expressions into an executable form, but each part implementing the executable form is locally simple. By using recursion, assembling the simple functions into a more function expressing a more complex rule also follows as being locally simple but gaining a usefully sophisticated behaviour through composition of simple parts. Each recursive call of the parser identifies one token for non-empty input and then recursively digests the rest.</p>
<p>The style of implementation of the generator of functions corresponding to json path expressions is reminiscent of a traditional parser generator, although rather than generating source, functions are dynamically composed. Reflecting on this, parser gens only went to source to break out of the ability to compose the expressive power of the language itself from inside the language itself. With a functional approach, assembly from very small pieces gives a similar level of expressivity as writing the logic out as source code.</p>
<p>Why could implement Function#partial via prototype. Why not going to. Is a shame. However, are using prototype for minimal set of polyfills. Not general purpose.</p>
<p>Different ways to do currying below:</p>
<pre class="sourceCode javascript"><code class="sourceCode javascript">
<span class="co">// function factory pattern (CITEME)</span>
<span class="kw">function</span> <span class="fu">foo</span>(a,b,c) {
   <span class="kw">return</span> <span class="kw">function</span> <span class="fu">partiallyCompleted</span>(d,e,f) {
   
      <span class="co">// may refer to partiallyCompleted in here</span>
   }
}

<span class="kw">function</span> <span class="fu">fooBar</span>(a,b,c,d,e,f) {
}

<span class="fu">partial</span>(fooBar, a,b);</code></pre>
<p>Partial completion is implemented using the language itself, not provided by the language.</p>
<p>Why would we choose 1 over the other? First simpler from caller side, second more flexible. Intuitive to call as a single call and can call self more easily.</p>
<p>In same cases, first form makes it easier to communicate that the completion comes in two parts, for example:</p>
<pre class="sourceCode javascript"><code class="sourceCode javascript"> <span class="fu">namedNodeExpr</span>(previousExpr, capturing, name, pathStack, nodeStack, stackIndex )</code></pre>
<p>There is a construction part (first 3 args) and a usage part (last three). Comsume many can only be constructed to ues consume 1 in second style because may refer to its own paritally completed version.</p>
<p>In first case, can avoid this: <code>consume1( partialComplete(consumeMany, previousExpr, undefined, undefined), undefined, undefined, pathStack, nodeStack, stackIndex);</code> because function factory can have optional arguments so don't have to give all of them</p>
<p>Function factory easier to debug. 'Step in' works. With partialCompletion have an awkward proxy function that breaks the programmer's train of thought as stepping through the code.</p>
<p>Why it is important to consider the frame of mind of the coder (CITEME: Hackers and Painters) and not just the elegance of the possible language expressions.</p>
<p>If implementing own functional caching, functional cache allows two levels of caching. Problematic though, for example no way to clear out the cache if memory becomes scarce.</p>
<p>Functional programming tends to lend better to minification than OO-style because of untyped record objects (can have any keys).</p>
<p>Lack of consistency in coding (don't write too much, leave to the conclusion)</p>
<p>Final consideration of coding: packaging up each unit to export a minimal interface. * Why minimal interfaces are better for minification</p>
<h2 id="the-mutability-problem"><a href="#the-mutability-problem"><span class="header-section-number">4.12</span> The mutability problem</a></h2>
<p>Javascript provides no way to decalre an object with 'cohorts' who are allowed to change it whereas others cannot - vars may be hidden via use of scope and closures (CITE: crockford) but attributes are either mutable or immutable.</p>
<p>Why this is a problem.</p>
<ul>
<li>bugs likely to be attributied to oboe because they'll be in a future <em>frame of execution</em>. But user error.</li>
</ul>
<p>Potential solutions:</p>
<ul>
<li>full functional-style immutability. Don't change the objects, just have a function that returns a new one with one extra property. Problem - language not optimised for this. A lot of copying. Still doesn't stop callback receiver from changing the state of hte object given. (CITE: optimisations other languages use)</li>
<li>immutable wrappers.</li>
<li>defensive cloning</li>
<li>defining getter properties</li>
</ul>
<h2 id="performance-implications-of-functional-javascript"><a href="#performance-implications-of-functional-javascript"><span class="header-section-number">4.13</span> Performance implications of functional javascript</a></h2>
<p>(perhaps move to background, or hint at it, eg &quot;although there are still some performance implications involved in a functional style, javascript may be used in a non-pure functional style&quot;) - with link to here</p>
<p>http://rfrn.org/~shu/2013/03/20/two-reasons-functional-style-is-slow-in-spidermonkey.html 9571 ms vs 504 ms</p>
<p>The performance degradation, even with a self-hosted forEach, is due to the JITâ€™s inability to efficiently inline both the closures passed to forEach</p>
<p>Lambda Lifting, currently not implemented in SpiderMonkey or V8: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.48.4346</p>
<p>The transformations to enable the above criteria are tedious and are surely the purview of the compiler. All thatâ€™s needed are brave compiler hackers</p>
<p>JS is much faster with &quot;monomorphic call sites&quot;</p>
<p>However, js execution time is not much of a problem,</p>
<h2 id="functions-over-constructors"><a href="#functions-over-constructors"><span class="header-section-number">4.14</span> functions over constructors</a></h2>
<p>What constructors are in js. Any function, but usually an uppercase initial char indicates that it is intended to be used as a constructor.</p>
<p>Inheritence is constructed using the language itself. While this is more flexible and allows each project to define a bespoke version of inherience to suit their particular needs or preferences, it also hampers portability more than an 'extends' keyword would.</p>
<blockquote>
<p>So far, the JavaScript community has not agreed on a common inheritance library (which would help tooling and code portability) and it is doubtful that that will ever happen. That means, weâ€™re stuck with constructors under ECMAScript 5. http://www.2ality.com/2013/07/defending-constructors.html</p>
</blockquote>
<p>Functions can be like Factories, gives me the flexability to chagne how something is created but by exposing a constructor are stuck with using 'new' to create an instance of exactly one type.</p>
<p>Dart has 'factory' constructors which are called like constructors but act like factory functions: (http://www.dartlang.org/docs/dart-up-and-running/contents/ch02.html#ch02-constructor-factory)</p>
<h2 id="targeting-node-and-the-browser"><a href="#targeting-node-and-the-browser"><span class="header-section-number">4.15</span> targeting node and the browser</a></h2>
<p>Node+browser To use Node.js and</p>
<p>Need to build an abstraction layer over xhr/xhr2/node. Can only work for packets in-order, for out-of-order packets something else happens.</p>
<p>Use best of the capabilities of each.</p>
<h2 id="packaging-the-library-as-a-single-distributable-file"><a href="#packaging-the-library-as-a-single-distributable-file"><span class="header-section-number">4.16</span> Packaging the library as a single distributable file</a></h2>
<div class="figure">
<img src="images/placeholder.png" alt="packaging of many javascript files into multiple single-file packages. The packages are individually targeted at different execution contexts, either browsers or node get from notebook, split sketch diagram in half" /><p class="caption">packaging of many javascript files into multiple single-file packages. The packages are individually targeted at different execution contexts, either browsers or node <em>get from notebook, split sketch diagram in half</em></p>
</div>
<ul>
<li>One file for browser and node is common.</li>
<li>say how this is done</li>
<li>why not doing this (adds bloat, inhibits micro-lib)</li>
<li>extra challenges</li>
<li>http adaptor is different</li>
<li>packaging is different</li>
<li>two distributable files, for node minification is not important so don't do to help debugging.</li>
</ul>
<p>Composition of several source files into a distributable binary-like text file</p>
<p>Why distributed javascript is more like a binary than a source file. Licencing implications? Would be (maybe) under GPL. Not so under BSD.</p>
<p>Inherent hiding by wrapping in a scope.</p>
<p>Names of functions and variable names which are provably not possible to reference are lost for the sake of reduction of size of the source.</p>
<p>Packaging for node or browser. No need to minify for node but concatenation still done for ease of inclusion in projects</p>
<pre class="sourceCode javascript"><code class="sourceCode javascript">typical pattern <span class="kw">for</span> packaging to work <span class="kw">in</span> either a <span class="ot">node</span>.<span class="fu">js</span> <span class="fu">server</span> <span class="fu">or</span> <span class="fu">a</span> <span class="fu">web</span> <span class="fu">browser</span></code></pre>
<p>Packaging for use in frameworks.</p>
<ul>
<li>Many frameworks already come with a wrapper arround the browser's inbuilt ajax capabilities</li>
<li><p>they don't add to the capabilities but present a nicer interface</p></li>
<li><p>I'm not doing it but others are ** browser-packaged version should be use agnostic and therefore amenable to packaging in this way</p></li>
</ul>
<p>Why uglify</p>
<ul>
<li>Covers whole language, not just a well-advised subset.</li>
<li>Closure compiler works over a subset of javascript rather than the whole language.</li>
</ul>
<p>Why not require. Bits on what rq is can go into B&amp;R section. <em>Some of this can move into 3_Background.md</em></p>
<ul>
<li>What it is</li>
<li>Why so popular</li>
<li>Why a loader is necessary - js doesn't come with an import statement</li>
<li>How it can be done in the language itself without an import statement</li>
<li>Meant more for AMD than for single-load code</li>
<li>Situations AMD is good for - large site, most visitors don't need all the code loaded</li>
<li>Depends on run-time component to be loaded even after code has been optimised</li>
<li>Small compatible versions exist that just do loading (almond)<br /></li>
<li>Why ultimately not suitable for a library like this - would require user to use Require before adopting it.</li>
</ul>
<p>Browserify is closer.</p>
<ul>
<li>Why it is better for some projects</li>
<li>Very nearly meets my needs</li>
<li>But http-compatability (https://github.com/substack/http-browserify), while complete enough, isn't compact enough to not push project over micro-library size</li>
</ul>
<p>Testing post-packaging for small set of smoke tests. Can't test everything, only through public API.</p>
<p>Uglify. Why not Google Closure Compiler.</p>
<h2 id="polyfilling"><a href="#polyfilling"><span class="header-section-number">4.17</span> polyfilling</a></h2>
<p>The decline of bad browsers. Incompatability less of a concern than it was.</p>
<p>Node doesn't require, built on v8.</p>
<p>http://www.jimmycuadra.com/posts/ecmascript-5-array-methods Unlike the new methods discussed in the first two parts, the methods here are all reproducible using JavaScript itself. Native implementations are simply faster and more convenient. Having a uniform API for these operations also promotes their usage, making code clearer when shared between developers.</p>
<p>Even when only used once, preferable to polyfill as a generic solution rather than offer a one-time implementation because it better splits the intention of the logic being presented from the mechanisms that that logic sits on and, by providing abstraction, elucidates the code.</p>
<h2 id="automated-testing"><a href="#automated-testing"><span class="header-section-number">4.18</span> automated testing</a></h2>
<div class="figure">
<img src="images/placeholder.png" alt="Relationship between various files and test libraries other half of sketch from notebook" /><p class="caption">Relationship between various files and test libraries <em>other half of sketch from notebook</em></p>
</div>
<p>How automated testing improves what can be written, not just making what is written more reliable.</p>
<p>TDD drives development by influencing the design - good design is taken as that which is amenable to testing rather than which describes the problem domain accurately or solves a problem with minimum resources. Amenable to testing often means split into many co-operating parts so that each part may be tested via a simple test.</p>
<p>Bt encourageing splitting into co-operating objects, TDD to a certain degree is anti-encapsulation. The public object that was extracted as a new concern from a larger object now needs public methods whereas before nothing was exposed.</p>
<div class="figure">
<img src="images/pyramid.png" alt="The testing pyramid is a common concept, relying on the assumption that verification of small parts provides a solid base from which to compose system-level behaviours. A Lot of testing is done on the low-level components of the system, whereas for the high-level tests only smoke tests are provided. " /><p class="caption">The testing pyramid is a common concept, relying on the assumption that verification of small parts provides a solid base from which to compose system-level behaviours. A Lot of testing is done on the low-level components of the system, whereas for the high-level tests only smoke tests are provided. </p>
</div>
<p>Jstd can serve example files but need to write out slowly which it has no concept of. Customistation is via configuration rather than by plug-in, but even if it were, the threading model is not suitable to create this kind of timed output.</p>
<p>Tests include an extremely large file twentyThousandRecords.js to test under stress</p>
<p>Why jstd's built in proxy isn't sufficient. An example of a typical Java webserver, features thread-based mutlithreading in which threads wait for a while response to be received.</p>
<p>Tests deal with the problem of &quot;irreducible complexity&quot; - when a program is made out of parts whose correct behaviour cannot be observed without all of the program. Allows smaller units to be verified before verifying the whole.</p>
<p>Conversely, automated testing allows us to write incomprehensible code by making us into more powerful programmers, it is possible building up layers of complexity one very small part at a time that we couldn't write in a simple stage. Clarity &gt; cleverness but cleverness has its place as well (intriducing new concepts)</p>
<p>Testing via node to give something to test against - slowserver. Proxy. JSTD not up to task. Shows how useful node is as a 'network glue'. The same as C was once described as a 'thin glue' [http://www.catb.org/esr/writings/taoup/html/ch04s03.html]. Transparent proxy is about 20 lines. Transparent enough to fool JSTD into thinking it is connecting directly to its server.</p>
<p>Node comes with very little built in (not even http) but relies on libraries written in the language itself to do everything. Could implement own http on top of sockets if wanted rather than using the provided one.</p>
<p>The test pyramid concept  fits in well with the hiding that is provided. Under the testing pyramid only very high level behaviours are tested as ??? tests. While this is a lucky co-incidence, it is also an unavoidable restriction. Once compiled into a single source file, the individual components are hidden, callable only from withing their closure. Hence, it would not be possible to test the composed parts individually post-concatenation into a single javascript file, not even via a workarround for data hiding such as found in Java's reflection. Whereas in Java the protection is a means of protecting otherwise addressable resources, once a function is trapped inside a javascript closure without external exposure it is not just protected but, appearing in no namespaces, inherently unreferenceable.</p>
<p>TDD fits well into an object pattern because the software is well composed into separate parts. The objects are almost tangible in their distinction as separate encapsulated entities. However, the multi-paradigm style of my implementation draws much fainter borders over the implementation's landscape.</p>
<p>Approach has been to the test the intricate code, then for wiring don't have tests to check that things are plumbed together correctly, rather rely on this being obvious enough to be detected via a smoke test.</p>
<p>A good test should be able to go unchanged as the source under test is refactored. Indeed, the test will be how we know that the code under test still works as intended. Experince tells me that testing that A listens to B (ie that the controller wires the jsonbuilder up to clarinet) produces the kind of test that 'follows the code arround' meaning that because it is testing implementation details rather than behaviours, whenever the implementation is updated the tests have to be updated too.</p>
<p>By testing individual tokens are correct and the use of those tokens as a wider expression, am testing the same thing twice. Arguably, redundant effort. But may simply be easier to write in that way - software is written by a human in a certain order and if we take a bottom-up approach to some of that design, each layer is easier to create if we first know the layers that it sits on are sound. Writing complex regular expressions is still programming and it is more difficult to test them completely when wrapped in rather a lot more logic than directly. For example, a regex which matches &quot;{a,b}&quot; or &quot;{a}&quot; but not &quot;{a,}&quot; is not trivial.</p>
<p>Can test less exhaustively on higher levels if lower ones are well tested, testing where it is easier to do whilst giving good guarantees.</p>
<p>Genuine data hiding gets in the way sometimes. Eg, token regexes are built from the combination of smaller regualar expressions for clarity (long regular expressions are concise but hard to read), and then wrapped in functions (why? - explain to generify interface) before being exposed. Because the components are hidden in a scope, they are not addressable by the tests and therefore cannot be directly tested. Reluctantly</p>
<p>One dilemma in implementing the testing is how far to test the more generic sections of the codebase as generic components. A purist approach to TDD would say</p>
<p>Could implement a resume function for if transmission stops halfway</p>
<pre class="sourceCode javascript"><code class="sourceCode javascript">   .<span class="fu">onError</span>( error ) {
      <span class="kw">this</span>.<span class="fu">resume</span>();
   }</code></pre>
<h2 id="inversion-of-control"><a href="#inversion-of-control"><span class="header-section-number">4.19</span> Inversion of Control</a></h2>
<p>Aim of creating a micro-library rules out building in a general-purpose IoC library.</p>
<p>However, can still follow the general principles.</p>
<p>Why the Observer pattern (cite: des patterns) lends itself well to MVC and inversion of control.</p>
<p>What the central controller does; acts as a plumber connecting the various parts up. Since oboe is predominantly event/stream based, once wired up little intervention is needed from the controller. Ie, A knows how to listen for ??? events but is unintested who fired them.</p>
<h2 id="stability-over-upgrades"><a href="#stability-over-upgrades"><span class="header-section-number">4.20</span> stability over upgrades</a></h2>
<p>why jsonpath-like syntax allows upgrading message semantics without causing problems <a href="#soa">SOA</a> how to guarantee non-breakages? could publish 'supported queries' that are guaranteed to work</p>
<h2 id="support-for-older-browsers"><a href="#support-for-older-browsers"><span class="header-section-number">4.21</span> support for older browsers</a></h2>
<p>Still works as well as non-progressive json Could be used for content that is inherently streaming (wouldn't make sense without streaming)</p>
<h2 id="suitability-for-databases"><a href="#suitability-for-databases"><span class="header-section-number">4.22</span> suitability for databases</a></h2>
<p>Databases offer data one row at a time, not as a big lump.</p>
<h2 id="weaknesses"><a href="#weaknesses"><span class="header-section-number">4.23</span> weaknesses</a></h2>
<p>implementation keeps 'unreachable' listeners difficult decidability/proof type problem to get completely right but could cover most of the easy cases</p>
<p>Parse time for large files spread out over a long time. Reaction to parsed content spread out over a long time, for example de-marshalling to domain objects. For UX may be preferable to have many small delays rather than one large one.</p>
<p>Doesn't support all of jsonpath. Not a strict subset of the language.</p>
<p>Rest client as a library is passing mutable objects to the caller. too inefficient to re-create a new map/array every time an item is not as efficient in immutability as list head-tail type storage</p>
<p>An imutability wrapper might be possible with defineProperty. Can't casually overwrite via assignment but still possible to do defineProperty again.</p>
<p>Would benefit from a stateless language where everything is stateless at all times to avoid having to program defensively.</p>
<h1 id="conclusion"><a href="#conclusion"><span class="header-section-number">5</span> Conclusion</a></h1>
<!---
**1 to 5 pages**
--->

<p>Doing things faster vs doing things earlier. &quot;Hurry up and wait&quot; approach to optimisation.</p>
<h2 id="development-methodology"><a href="#development-methodology"><span class="header-section-number">5.1</span> Development methodology</a></h2>
<p>Did it help?</p>
<p>Switched several times. Could have started with winning side? Tension between choosing latest and greatest (promising much) or old established solution alraedy experienced with but with known problems. Judging if problems will become too much of a hinderence and underestimating the flaws. JSTD was yesterday's latest and greatest but Karma genuinely is great. In end, right solution was found despite not being found in most direct way.</p>
<p>Packaging was a lot of work but has delivered the most concise possible library.</p>
<h2 id="size"><a href="#size"><span class="header-section-number">5.2</span> Size</a></h2>
<div class="figure">
<img src="images/placeholder.png" alt="A pie chart showing the sizes of the various parts of the codebase" /><p class="caption">A pie chart showing the sizes of the various parts of the codebase</p>
</div>
<p>Comment on the size of the libraray</p>
<h2 id="handling-invalid-input"><a href="#handling-invalid-input"><span class="header-section-number">5.3</span> Handling invalid input</a></h2>
<p>Invalid jsonpaths made from otherwise valid clauses (for example two roots) perhaps could fail early, at compile time. Instead, get a jsonPath that couldn't match anything. Invalid syntax is picked up.</p>
<p>Same pattern could be extended to XML. Or any tree-based format. Text is easier but no reason why not binary applications.</p>
<p>Not particularly useful reading from local files.</p>
<p>Does not save memory over DOM parsing since the same DOM tree is built. May slightly increase memory usage by utilising memory earlier that would otherwise be dept dormant until the whole transmission is received but worst case more often a concern than mean.</p>
<p>Implementation in a purely functional language with lazy evaluation: could it mean that only the necessary parts are computed? Could I have implemented the same in javascript?</p>
<p>Would be nice to: * discard patterns that can't match any further parts of the tree * discard branches of the tree that can't match any patterns * just over the parsing of branches of the tree that provably can't match any of the patterns</p>
<h2 id="comparative-usages"><a href="#comparative-usages"><span class="header-section-number">5.4</span> Comparative usages</a></h2>
<p>Interesting article from Clarinet: http://writings.nunojob.com/2011/12/clarinet-sax-based-evented-streaming-json-parser-in-javascript-for-the-browser-and-nodejs.html</p>
<p>In terms of syntax: compare to SAX (clarinet) for getting the same job done. Draw examples from github project README. Or from reimplementing Clarinet's examples.</p>
<p>Consider:</p>
<ul>
<li>Difficulty to program</li>
<li>Ease of reading the program / clarity of code</li>
<li>Resources consumed</li>
<li>Performance (time) taken</li>
<li>about the same. Can react equally quickly to io in progress, both largely io bound.</li>
<li>Is earlier really faster?</li>
</ul>
<h2 id="community-reaction"><a href="#community-reaction"><span class="header-section-number">5.5</span> Community reaction</a></h2>
<p>Built into Dojo Followers on Github Being posted in forums (hopefully also listed on blogs) No homepage as of yet other than the Github page</p>
<h1 id="bibliography"><a href="#bibliography"><span class="header-section-number">6</span> Bibliography</a></h1>
<h1 id="appendix"><a href="#appendix"><span class="header-section-number">7</span> Appendix</a></h1>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>B. L. Whorf (1956): Language, Thought and Reality (ed. J. B. Carroll). Cambridge, MA: MIT Press<a href="#fnref1">â†©</a></p></li>
<li id="fn2"><p>E. Sapir (1958): Culture, Language and Personality (ed. D. G. Mandelbaum). Berkeley, CA: University of California Press<a href="#fnref2">â†©</a></p></li>
<li id="fn3"><p>Anthony Ralston (2000). Encyclopedia of computer science. Nature Pub. Group. ISBN 978-1-56159-248-7.<a href="#fnref3">â†©</a></p></li>
<li id="fn4"><p>Fielding, R. T.; Taylor, R. N. (2000). Principled design of the modern Web architecture. pp. 407â€“416.<a href="#fnref4">â†©</a></p></li>
<li id="fn5"><p>http://uxdesign.smashingmagazine.com/2013/05/03/infinite-scrolling-get-bottom/<a href="#fnref5">â†©</a></p></li>
<li id="fn6"><p>Eric Reis (2011), The Lean Startup: How Today's Entrepreneurs Use Continuous Innovation to Create Radically Successful Businesses. Crown Business Publishing<a href="#fnref6">â†©</a></p></li>
</ol>
</div>
